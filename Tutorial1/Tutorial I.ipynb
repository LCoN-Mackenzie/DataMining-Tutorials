{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%; margin-left:auto; margin-right:auto;\">\n",
    "    <img src=./img/mackenzie_logo.png style=\"height:70px; float:left; margin-top:0px;\"/>\n",
    "    <img src=./img/intel_logo.png style=\"height:70px; float:right; margin-top:0px;\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-top:120px;\">\n",
    "    <h1 style=\"text-align:center;\">AN INTRODUCTION TO MACHINE LEARNING WITH INTEL</h1>\n",
    "</div>\n",
    "<div style=\"margin-top:1px;\">\n",
    "    <h1 style=\"text-align:center;\">TUTORIAL 1: Basic Concepts and Intel pyDAAL Data Management and Preprocessing</h1>\n",
    "</div>\n",
    "\n",
    "<div style=\"width:100%; margin-top:50px;\">\n",
    "    <p style=\"text-align:center;\">Ana Carolina E. S. Lima, Rafael M. Santos, Calebe Bianchini, Leandro Nunes de Castro</p>\n",
    "    <p style=\"text-align:center;\">aceslima@gmail.com; santosrm@outlook.com.br; calebe.bianchini@mackenzie.br; lnunes@mackenzie.br</p>\n",
    "    <p style=\"text-align:center;\">Natural Computing and Machine Learning Laboratory (LCoN)</p>\n",
    "    <p style=\"text-align:center;\">Graduate Program in Electrical Engineering and Computing (PPGEEC)</p>\n",
    "    <p style=\"text-align:center;\">Mackenzie Presbiteryan University</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Machine learning emerged as an independent area of research and application in the mid-1990s, but its origins in mathematics, statistics, and computing are much earlier. The area has also gained evidence in recent years after the term Big Data was coined. Machine learning is the central element responsible for the analytical part of Big Data, that is, the preparation and analysis of huge data sets. With the new nomenclature, even the professionals who work in the area have gained a new name: data analysts, or data scientists, and these professionals are increasingly demanded and well paid, especially as the volume of data produced grows exponentially, to the point that in short periods of time more data are generated than in many centuries of human history. For this growth there seems to be no limits, and the academic and commercial opportunities of the area also arise in great variety, velocity and volume. This paper is the first of a series of four Tutorials dedicated to the introduction of Machine Learning. In this tutorial you will learn how to load and pre-process using an Intel DAAL data structure called *Numeric Table*. We will explain what is the pre-processing step, introduce Python DAAL (pyDAAL), and then present how to manipulate data using this library. In the following tutorials you will learn how to perform some standard data analysis, to create and validate predictive models, and then some applications involving decision making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Intel Machine Learning Centers of Excellence\n",
    "\n",
    "Intel has a complete Machine Learning (ML) and Artificial Intelligence (AI) portfolio, world-class processors, and experience in driving the world's major computing transformations. Intel works in partnership with universities and companies to develop Machine Learning Centers of excellence (CoE) in Brazil. The CoEs have the objective of collaborating with the industry and the scientific community in the solution of complex problems through the use of Artificial Intelligence. Intel also offers, through this initiative, workshops, taught by universities, the entire scientific community and software developers to highlight the full potential of artificial intelligence. The centers also collaborate with Intel customers on proof-of-concepts, pilot projects and solutions for different verticals in the industry.\n",
    "\n",
    "Intel's performance in machine learning aims to accelerate the data analysis power and speed through a continuous evolution of high-performance computing, Big Data, and artificial intelligence. Intel provides complete tools to bring performance to Big Data applications such as Intel Python Distribution, Intel Math Kernel Library and Intel Data Analytics Acceleration Library (DAAL), which are key to extracting all parallelism from Xeon and Xeon Phi processors.\n",
    "\n",
    "The Machine Learning CoEs are part of a global initiative of the company, called Intel AI Academy, which aims to provide training in Artificial Intelligence for students of STEM courses (Science, Technology, Engineering and Mathematics and related) and develop artificial intelligence applications. Big Data technologies that make use of Artificial Intelligence algorithms often demand high computational power and high performance machines. The centers of excellence also have tools such as the Intel Deep Learning SDK, which streamlines the application development process, increase productivity, and accelerate time-to-market in deep neural network applications.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. An Introduction to Machine Learning and the Intel DAAL\n",
    "\n",
    "The number of Internet users worldwide has grown from 16 million people in 1995 to approximately 2.8 billion in 2013; the number of articles published only in English on Wikipedia raised from 500,000 in 2005 to almost 4.4 million in 2013; the time it took for the radio to reach an audience of 50 million people was 38 years, while the TV took 13 years and the Internet only lasted four years to reach the same number of people and the number of daily searches on Google exceeds five billion.\n",
    "\n",
    "In 1965 Gordon Moore, an Intel co-founder, published a paper in which he noted that the number of components in an integrated circuit (IC) was doubling approximately every year since its invention in 1958, and that rate would remain at least ten years. In 1975, Moore updated his estimate for periods of two years instead of one year. This high growth rate in the number of IC components is directly related to the processing speed and memory capacity of computers and has also served as a goal for the computer hardware industry.\n",
    "\n",
    "This rapid growth in data generation and storage along with the evolution of computers has led to the emergence of several areas of research and development, such as Knowledge Discovery in Databases (KDD), Data Mining, Machine Learning, Natural Computing, and Big Data.\n",
    "\n",
    "The term data mining (DM) was coined as an allusion to the mining process, since it explores a database (mine) using appropriate algorithms (tools) to obtain knowledge (precious minerals). The data are unstructured and meaningless symbols or signs, such as values in a table, and the information is contained in the descriptions, adding meaning and utility to the data, such as the height of a person. Finally, knowledge is something that allows decision making, so, for example, knowing that it will rain over the weekend can influence your decision to travel or not to the beach. The terminology KDD and DM has been used interchageably in the literature, and this will be our use here as well.\n",
    "\n",
    "Data mining tasks can be classified into two categories: (1) descriptive: characterize the general properties of the data; and (2) predictive: they make inferences from the data aiming at predictions. Machine learning proposes learning based on data and is the core of data mining. The main predictive tasks are classification, estimation, clustering, association and anomaly detection.\n",
    "\n",
    "## 2.1 Descriptive Data Analysis\n",
    "Machine learning algorithms are powerful tools to discover knowledge in databases. However, an initial stage of the mining process that does not require a high level of sophistication is the descriptive analysis of data, i.e., the use of tools capable of measuring, exploring and describing characteristics intrinsic to the data. Specifically, these analyzes allow us to investigate the frequency distribution, central tendency, variation, relative position and association measures. In addition, elementary visualization techniques are also employed for a better understanding of the nature and distribution of data.\n",
    "\n",
    "## 2.2 Prediction: Classification and Estimation\n",
    "Prediction is a terminology used to refer to the construction and use of a model to evaluate the class of an unlabeled object or to estimate the value of one or more attributes of a given object. In the first case, we call it a classification task and, in the second case, we call it regression (in statistics) or simply estimation. From this perspective, classification and estimation are the two main types of prediction problems, and classification is used to predict discrete values, while estimation is used to predict continuous values.\n",
    "\n",
    "## 2.3 Group Analysis\n",
    "Clustering is the name given to the process of separating (partitioning or segmenting) a set of objects into groups (clusters) of similar objects. Unlike the classification task, data clustering considers unlabeled input data, that is, the group (class) to which each input data (object) belongs is not known a priori. The process of grouping (or clustering) is usually used to identify such groups and, therefore, each group formed can be viewed as a class of objects. Since the training data class labels are not known a priori, this process is called unsupervised training (or unsupervised learning).\n",
    "\n",
    "## 2.4 Association\n",
    "In group and predictive analyzes, the goal is usually to find relationships (groups, classes, or estimates) between the objects in the database. However, there are several practical applications in which the objective is to find relationships between attributes (or variables), not between objects. Association analysis, also known as association rule mining, corresponds to the discovery of association rules that present attribute values that co-occur in a database. This type of analysis is typically used in marketing actions and in the study of transactional databases.\n",
    "\n",
    "## 2.5 Anomaly Detection\n",
    "A database can contain objects that do not follow the behavior or do not have the common characteristics of the data or a model that represents them. These data are known as anomalies or outliers. Most mining tools rule out anomalies - for example, noises or exceptions - but in some applications, such as fraud detection, rare events may be more informative than those that occur regularly. Anomalies can be detected in a number of ways, including by using statistical methods that assume a probability distribution or model of the data, or distance measures by which objects substantially distant from the others are considered anomalies.\n",
    "\n",
    "## 2.6 The KDD Process\n",
    "The knowledge discovery in databases process consists of a series of steps, from data preprocessing to the postpreprocessing of results, as shown in Figure 1. The purpose of preprocessing is to transform the data into an appropriate format for subsequent analysis, it may involve fusing data from multiple sources, cleaning data to remove noise, duplicate objects, missing values, reduce dimensionality, and other steps. After preprocessing, the data are ready to be analyzed by performing tasks like classification, regression, clustering, anomaly detection, etc.\n",
    "\n",
    "![The process of knowledge discovery in databases](img/dm_process.png)\n",
    "<p style=\"text-align:center;\"><strong>Figure 1.</strong> The process of knowledge discovery in databases (Source: http://www.zentut.com/data-mining/what-is-data-mining/).</p>\n",
    "\n",
    "\n",
    "## 2.7 Types of Data and Attributes\n",
    "\n",
    "In a simplified way, data are quantitative or qualitative values associated with some attributes. With respect to the structure, they can be structured, when the data resides in fixed fields in a file; semistructured, when the data does not have the complete structure of a data model, it is also not totally unstructured; and unstructured, when there is no data model, the data are not organized in a predefined manner, or do not reside in defined locations. This terminology usually refers to free texts, images, videos, sounds, web pages, PDF files, among others.\n",
    "\n",
    "In the case of structured data, each row of the data table usually corresponds to an object, an example, an instance, a record, an input vector, an observation, or a pattern. Each column corresponds to an attribute, a feature, an entry, or a variable. The value of an attribute of a given object is a measure of the quantity of that attribute, which can be numeric or categorical.\n",
    "\n",
    "\n",
    "## 2.8 The Intel Data Analytics Acceleration Library (DAAL)\n",
    "The Intel Data Analytics Accerelation Library (DAAL) is a highly optimized library of computationally intensive routines supporting the Intel architecture, which provides a rich set of algorithms, ranging from the most basic descriptive statistics for datasets to more advanced data mining and machine learning algorithms. Figure 2 illustrates the main algorithms available in Intel DAAL. Note that DAALs flow is basically the standard KDD flow, but it divides the data mining block into three parts: Analysis, Modeling and Validation. The result of the whole process can be used for Decision Making.\n",
    "\n",
    "![Main processes delivered by Intel DAAL](img/daal-flow.png)\n",
    "<p style=\"text-align:center;\"><strong>Figure 2.</strong> Main processes delivered by Intel DAAL (Source: https://software.intel.com/en-us/blogs/daal)</p>\n",
    "\n",
    "To work with the optimized features provided by DAAL the data must be readable in a library format, such as *NumericTable*, because Intel DAAL provides a set of customizable interfaces to operate with out-of-memory and in-memory data in different usage scenarios, which includes batch processing, online processing, and distributed processing, as well as more complex scenarios, such as a combination of online and distributed processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Data Management\n",
    "\n",
    "One of the main concepts in Data Management using Intel DAAL is that of a data set. A data set is a collection of data with a defined structure that characterizes an item being analyzed and modeled. More specifically, an object is characterized by a set of attributes (features), which form a Feature Vector of dimension p (see Figure 3). Multiple feature vectors form a set of observations (or objects) of size n. Intel DAAL defines a tabular view of a data set where table rows represent observations and columns represent features.\n",
    "\n",
    "![Data set structure in Intel DAAL](img/dataset_daal.png)\n",
    "<p style=\"text-align:center;\"><strong>Figure 3.</strong> Data set structure in Intel DAAL (Source: https://software.intel.com/en-us/node/564578)</p>\n",
    "\n",
    "Figure 4 shows a typical data flow when one works with Intel DAAL. First, we normally select an appropriate data source, like files, ODBC, stream data, etc. Then, we need to transform the raw data into a numeric representation supported by the library layout (Numeric Table). A *Numeric Table* is a key interface to operate on numeric in-memory data. Intel DAAL supports several important cases of a numeric data layout: homogeneous tables, arrays of structures, and structures of arrays, as well as Compressed Sparse Row (CSR) encoding schemes for sparse data. We can also perform filtering and normalization with the DAAL library, however, we will see that some preprocessing steps must be performed before transforming the data into a numeric table. The data sources support categorical, ordinal, and continuous features. It means that data sources can automatically transform non-numeric categorical and ordinary data into a numeric representation. When the structure of your raw data is more complex or when the default transformation mechanism does not fit your needs, you may customize the data source by implementing a custom derivative class. \n",
    "\n",
    "![Data flow within Intel DAAL](img/datamanagement_daal.png)\n",
    "<p style=\"text-align:center;\"><strong>Figure 4.</strong> Data flow within Intel DAAL (Source: https://software.intel.com/en-us/node/564578)</p>\n",
    "\n",
    "Now, the library streams in-memory data to the algorithm by blocks to improve data locatity and converts numeric formats into a  smaller set of numeric format for effective vectorization. So, effective data management requires effectively performing the following operations:\n",
    "\n",
    "1. Raw data acquisition, filtering, and normalization with data source interfaces.\n",
    "2. Data Conversion to a numeric representation for numeric tables.\n",
    "3. Data streaming from a numeric table to an algorithm.\n",
    "\n",
    "## 3.1 Numeric Tables\n",
    "\n",
    "Numeric tables are fundamental components of in-memory numeric data processing. Intel DAAL supports heterogeneous and homogeneous numeric tables for dense and sparse data, as follows:\n",
    "\n",
    "* Heterogeneous: Array Of Structures (AOS) and Structure Of Arrays (SOA)\n",
    "* Homogeneous: Dense, dense matrix, symmetric matrix, triangular matrix and Condensed Sparse Row (CSR)\n",
    "\n",
    "In this tutorial we focus on dense homogeneous numeric table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneous Numeric Tables\n",
    "We use homogeneous numeric tables (HomogenNumericTable class) when all the features are of the same basic data type. The CSRNumericTable class for a special version of a homogeneous numeric table that encodes sparse data, that is, the data with a significant number of zero elements. For example, we create a numpy array and transform into Homogen Numeric Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1, Columns: 6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from daal.data_management import HomogenNumericTable\n",
    "from daal import data_management\n",
    "\n",
    "\n",
    "# Create an array with six elements\n",
    "X = np.array([1., 2., 3., 4., 5., 6.])\n",
    "# We reshape the array because HomogenNumericTable only takes array with a fully defined dimension\n",
    "X = X.reshape(1, 6)\n",
    "\n",
    "# Now we transform the array using HomogenNumericTable\n",
    "X_transformed = HomogenNumericTable(X)\n",
    "print(\"Rows: %s, Columns: %s\" % (X_transformed.getNumberOfRows(), X_transformed.getNumberOfColumns()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "\n",
    "This section shows how to visualize the numeric table data. We need to use a BlockDescriptor and fill it with data from the NumericTable. This BlockDescriptor represents the data in the contiguous raw-major layout with the number of rows specified in the method and number of columns specified in the numeric table\n",
    "\n",
    "The following parameters are necessary to create the BlockDescriptor:\n",
    "* First row of NumericTable to load (firstReadRow)\n",
    "* Number of rows that will be loaded into the block (nObjects)\n",
    "* readOnly or readWrite\n",
    "* BlockDescription created (block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18216877  0.5752282   0.63169631  0.9249608 ]\n",
      " [ 0.99012854  0.35780605  0.37556282  0.54253661]\n",
      " [ 0.16036839  0.75798223  0.76718671  0.86210754]\n",
      " [ 0.33841689  0.96377547  0.08717084  0.98759424]]\n"
     ]
    }
   ],
   "source": [
    "from daal.data_management import BlockDescriptor, readOnly, readWrite\n",
    "\n",
    "# Random array\n",
    "data = np.random.random((4,4)) \n",
    "dataset = HomogenNumericTable(data)\n",
    "\n",
    "firstReadRow = 0\n",
    "nObjects = dataset.getNumberOfRows()\n",
    "\n",
    "block = BlockDescriptor()\n",
    "dataset.getBlockOfRows(firstReadRow, nObjects, readOnly, block)\n",
    "\n",
    "numpy_array = block.getArray()\n",
    "print(numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving values of a Feature (by columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+000]\n",
      " [  0.00000000e+000]\n",
      " [  2.55883664e-307]\n",
      " [ -2.54920513e-056]]\n"
     ]
    }
   ],
   "source": [
    "featureIdx = 0\n",
    "firstRow = 0\n",
    "nObjects = dataset.getNumberOfRows()\n",
    "\n",
    "dataset.getBlockOfColumnValues(featureIdx, firstRow,\n",
    "                                 nObjects, readOnly, block)\n",
    "\n",
    "dataset.releaseBlockOfColumnValues(block)\n",
    "print(block.getArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving objects (by rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18216877  0.99012854  0.16036839  0.33841689]]\n"
     ]
    }
   ],
   "source": [
    "firstReadRow = 0\n",
    "numRowRead = 1\n",
    "dataset.getBlockOfRows(firstReadRow, numRowRead, readOnly, block)\n",
    "print(block.getArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading and Preprocessing Data\n",
    "\n",
    "This section brings a general overview of how to load data using the Scikit-Learn, and then some basic pre-processing steps.\n",
    "\n",
    "## 4.1 Loading Datasets\n",
    "\n",
    "### Loading a dataset from the Scikit-Learn\n",
    "\n",
    "The Scikit Learn comes with some small standard datasets to getting started with the library. In this example, we use the Diabetes dataset. The load dataset contains two sets: training and target data. Look at the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03807591  0.05068012  0.06169621 ..., -0.00259226  0.01990842\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ..., -0.03949338 -0.06832974\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ..., -0.00259226  0.00286377\n",
      "  -0.02593034]\n",
      " ..., \n",
      " [ 0.04170844  0.05068012 -0.01590626 ..., -0.01107952 -0.04687948\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...,  0.02655962  0.04452837\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ..., -0.03949338 -0.00421986\n",
      "   0.00306441]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from daal.data_management import HomogenNumericTable\n",
    "\n",
    "dataset = datasets.load_diabetes()\n",
    "# After loading the dataset we need to transform the data into a NumPy continuous array\n",
    "data = np.ascontiguousarray(dataset.data)\n",
    "print(data)\n",
    "data = HomogenNumericTable(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a dataset from CSV\n",
    "\n",
    "When the dataset has all attributes of the same type (e.g., numeric), the data can be imported in a simple manner using the Numpy or Pandas libraries. However, when there are different types of attributes, a different import mechanism has to be used, as will be illustrated in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes with the same basic data type (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from daal.data_management import HomogenNumericTable\n",
    "\n",
    "dataset_pandas = pd.read_csv(\"./dataset/winequality-red.csv\", delimiter=\";\")\n",
    "dataset_numpy = np.ascontiguousarray(dataset_pandas, dtype=np.double)\n",
    "dataset = HomogenNumericTable(dataset_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes with the same basic data type (NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from daal.data_management import HomogenNumericTable\n",
    "\n",
    "dataset = np.genfromtxt(\"./dataset/winequality-red.csv\", delimiter=\";\")\n",
    "dataset_numpy = np.ascontiguousarray(dataset, dtype=np.double)\n",
    "dataset = HomogenNumericTable(dataset_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes with different data types\n",
    "In this case, we first need to instantiate a new FileDataSource. We have to feed the data source with the name of the CSV file. We also have to define whether or not the dataset metadata (Dictionary) and the NumericTable are automatically created from the CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects: 48843\n"
     ]
    }
   ],
   "source": [
    "from daal.data_management import FileDataSource\n",
    "from daal.data_management import HomogenNumericTable\n",
    "from daal.data_management import DataSourceIface\n",
    "from daal.data_management import NumericTableIface\n",
    "\n",
    "dataset_filename = './dataset/adult.csv'\n",
    "\n",
    "dataset = FileDataSource(dataset_filename, \n",
    "                            DataSourceIface.doAllocateNumericTable,\n",
    "                            DataSourceIface.doDictionaryFromContext)\n",
    "\n",
    "objetcs = dataset.loadDataBlock()\n",
    "\n",
    "print(\"Number of objects: %s\" % objetcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading a dataset from URL (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", delimiter=\";\")\n",
    "dataset_numpy = np.ascontiguousarray(dataset, dtype=np.double)\n",
    "dataset = HomogenNumericTable(dataset_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading a dataset from URL (NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "dataset = np.genfromtxt(url, delimiter=\";\")\n",
    "\n",
    "dataset_numpy = np.ascontiguousarray(dataset, dtype=np.double)\n",
    "dataset = HomogenNumericTable(dataset_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.2 Data Preprocessing\n",
    "\n",
    "Preprocessing, also known as database preparation, manipulates and transforms the raw data so that the knowledge contained in it can be more easily and correctly obtained. The best way to pre-process data depends on three central factors: database problems (incompleteness, inconsistency, and noise); what answers are sought from the data, i.e. what problem should be solved; and how the data mining techniques that will be employed operate. These three factors are almost always interrelated. The main preprocessing tasks are:\n",
    "* Data cleaning: filling in missing values, smoothing noisy data, identifying or removing outliers, and resolving inconsistencies.\n",
    "* Data integration: using multiple databases, data cubes, or files.\n",
    "* Data transformation: normalizing and aggregating data.\n",
    "* Data reduction: reducing the volume or the dimension of the data.\n",
    "* Data discretization: replacing numerical attributes with nominal ones (as part of data reduction).\n",
    "\n",
    "This tutorial will focus on the imputation of missing values and data normalization, which are the most common preprocessing methods used in the literature.\n",
    "\n",
    "To deal with missing values one can:\n",
    "* Ignore the tuple: usually done when the class label is missing.\n",
    "* Use the attribute mean (or mode for nominal attributes) to impute the missing value.\n",
    "* Use the attribute mean (or mode for nominal attributes) of all samples belonging to the same class.\n",
    "* Predict the missing value by using a learning algorithm.\n",
    "\n",
    "Data normalization, by contrast, standardize the range of an attribute. The main forms of doing that are: \n",
    "* Scaling the attribute values to fall within a specified range (e.g. [0, 1]).\n",
    "* Scaling the attribute values by using a standardized value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values Imputation (Pandas)\n",
    "\n",
    "In this example we will use the Pandas library to load the dataset and check for missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                      False\n",
      "fixed.acidity            True\n",
      "volatile.acidity        False\n",
      "citric.acid              True\n",
      "residual.sugar           True\n",
      "chlorides                True\n",
      "free.sulfur.dioxide     False\n",
      "total.sulfur.dioxide     True\n",
      "density                  True\n",
      "pH                       True\n",
      "sulphates               False\n",
      "alcohol                 False\n",
      "quality                 False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from daal.data_management import HomogenNumericTable\n",
    "dataset = pd.read_csv(\"./dataset/wineQualityReds.csv\", delimiter=\",\")\n",
    "\n",
    "# Check if the base contains empty values\n",
    "check_empty = dataset.isnull().any()\n",
    "print(check_empty)\n",
    "\n",
    "# Impute values by mean\n",
    "mean = dataset.mean()\n",
    "dataset_impute = dataset.fillna(dataset.mean())\n",
    "\n",
    "# Remove objects\n",
    "dataset_remove = dataset.dropna()\n",
    "\n",
    "dataset_numpy = np.ascontiguousarray(dataset_impute, dtype=np.double)\n",
    "data_daal = HomogenNumericTable(dataset_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values Imputation (Sklearn)\n",
    "In this example we will use the Scikit-learn library to load the dataset and check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.          2.        ]\n",
      " [ 6.          3.66666667]\n",
      " [ 7.          6.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "from daal.data_management import HomogenNumericTable\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "\n",
    "Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)\n",
    "dataset = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "dataset = imp.transform(dataset)\n",
    "print(dataset)  \n",
    "\n",
    "dataset_numpy = np.ascontiguousarray(dataset)\n",
    "dataset = HomogenNumericTable(dataset_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling attributes to a range (z-score)\n",
    "In the z-score scaling (normalization), also known as zero-mean normalization, the values of an attribute are normalized based on the mean and standard deviation of the attribute. This normalization method is useful when the actual maximum and minimum values of an attribute are unknown or when there are outliers dominating the max-min normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+00   7.40000000e+00   7.00000000e-01   0.00000000e+00\n",
      "    1.90000000e+00   7.60000000e-02   1.10000000e+01   3.40000000e+01\n",
      "    9.97800000e-01   3.51000000e+00   5.60000000e-01   9.40000000e+00\n",
      "    5.00000000e+00]\n",
      " [  2.00000000e+00   7.80000000e+00   8.80000000e-01   0.00000000e+00\n",
      "    2.60000000e+00   9.80000000e-02   2.50000000e+01   6.70000000e+01\n",
      "    9.96800000e-01   3.20000000e+00   6.80000000e-01   9.80000000e+00\n",
      "    5.00000000e+00]\n",
      " [  3.00000000e+00   7.80000000e+00   7.60000000e-01   4.00000000e-02\n",
      "    2.30000000e+00   9.20000000e-02   1.50000000e+01   5.40000000e+01\n",
      "    9.97000000e-01   3.26000000e+00   6.50000000e-01   9.80000000e+00\n",
      "    5.00000000e+00]\n",
      " [  4.00000000e+00   1.12000000e+01   2.80000000e-01   5.60000000e-01\n",
      "    1.90000000e+00   7.50000000e-02   1.70000000e+01   6.00000000e+01\n",
      "    9.98000000e-01   3.16000000e+00   5.80000000e-01   9.80000000e+00\n",
      "    6.00000000e+00]]\n",
      "-----\n",
      "[[ 0.36327953  0.40544179  0.83306864 -0.80075408]\n",
      " [-1.09054764 -0.32646947 -1.14730098  0.89992228]\n",
      " [ 1.20454855  1.12797183  0.84280055 -0.92822385]\n",
      " [-0.47728045 -1.20694415 -0.5285682   0.82905565]]\n"
     ]
    }
   ],
   "source": [
    "import daal.algorithms.normalization.zscore as zscore\n",
    "from daal.data_management import HomogenNumericTable\n",
    "from daal.data_management import BlockDescriptor, readOnly\n",
    "\n",
    "# Random array\n",
    "data = np.random.random((4,4)) \n",
    "dataset = HomogenNumericTable(data)\n",
    "\n",
    "block = BlockDescriptor()\n",
    "data_daal.getBlockOfRows(0, 4, readOnly, block)\n",
    "np_array = block.getArray()\n",
    "print(np_array)\n",
    "\n",
    "print(\"-----\")\n",
    "\n",
    "norm = zscore.Batch()\n",
    "norm.input.set(zscore.data, dataset)\n",
    "res = norm.compute()\n",
    "data_norm = res.get(zscore.normalizedData)\n",
    "\n",
    "block = BlockDescriptor()\n",
    "data_norm.getBlockOfRows(0, 4, readOnly, block)\n",
    "np_array = block.getArray()\n",
    "print(np_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling attributes to a range (Max-Min)\n",
    "The max-min normalization performs a linear transformation on the original data. Assume that max and min are, respectively, the maximum and minimum values of a given attribute. The max-min normalization maps a given value into a new value in a specified domain. The most frequent application of this normalization is to put all the attributes of a database under the same range of values, for example in the interval [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.       7.4      0.7      0.       1.9      0.076   11.      34.\n",
      "    0.9978   3.51     0.56     9.4      5.    ]\n",
      " [  2.       7.8      0.88     0.       2.6      0.098   25.      67.\n",
      "    0.9968   3.2      0.68     9.8      5.    ]]\n",
      "-----\n",
      "[[ 0.          0.24778761  0.39726027  0.          0.06849315  0.10684474\n",
      "   0.14084507  0.09893993  0.56754772  0.60629921  0.13772455  0.15384615\n",
      "   0.4       ]\n",
      " [ 0.00062578  0.28318584  0.52054795  0.          0.11643836  0.14357262\n",
      "   0.33802817  0.2155477   0.49412628  0.36220472  0.20958084  0.21538462\n",
      "   0.4       ]]\n"
     ]
    }
   ],
   "source": [
    "import daal.algorithms.normalization.minmax as minmax\n",
    "from daal.data_management import HomogenNumericTable, BlockDescriptor, readOnly\n",
    "\n",
    "# Random array\n",
    "data = np.random.random((2,2)) \n",
    "dataset = HomogenNumericTable(data)\n",
    "\n",
    "block = BlockDescriptor()\n",
    "data_daal.getBlockOfRows(0, 2, readOnly, block)\n",
    "np_array = block.getArray()\n",
    "print(np_array)\n",
    "\n",
    "print(\"-----\")\n",
    "\n",
    "norm = minmax.Batch()\n",
    "norm.input.set(minmax.data, data_daal)\n",
    "res = norm.compute()\n",
    "data_norm = res.get(minmax.normalizedData)\n",
    "\n",
    "block = BlockDescriptor()\n",
    "data_norm.getBlockOfRows(0, 2, readOnly, block)\n",
    "np_array = block.getArray()\n",
    "print(np_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "\n",
    "- TAPSCOTT, D. Grown Up Digital. Nova York: McGraw-Hill, 2009.\n",
    "- MOORE, G. E. (1965), “Cramming more Components onto Integrated Circuits”. Electronics, 38 (8), p. 114-117.\n",
    "- MITCHELL, T. M. Machine Learning. Nova York: McGraw-Hill, 1997.\n",
    "- ALPAYDIN, E. Introduction to Machine Learning. 2. ed. Cambridge: MIT Press, 2009.\n",
    "- DE CASTRO, L. N.; FERRARI, D. G. (2016), An Introduction to Data Mining: Basic Concepts, Algorithms, and Applications, (in Portuguese), Saraiva.\n",
    "- https://software.intel.com/en-us/blogs/daal\n",
    "- http://www.cs.ccsu.edu/~markov/ccsu_courses/datamining-3.html\n",
    "- https://software.intel.com/en-us/node/682181"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
